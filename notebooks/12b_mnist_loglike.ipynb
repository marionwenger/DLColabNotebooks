{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marionwenger/DLColabNotebooks/blob/main/notebooks/12b_mnist_loglike.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbTYFRhJoaBu"
      },
      "source": [
        "# Calculation of the cross entropy loss (NLL) for a classification tasks\n",
        "\n",
        "\n",
        "**Goal:** In this notebook you will use Keras to set up a CNN for classification of MNIST images and calculate the cross entropy before the CNN was trained. You will use basic numpy functions to calculate the loss that is expected from random guessing and see that an untrained CNN is not better than guessing.\n",
        "\n",
        "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it.\n",
        "\n",
        "**Dataset:** You work with the MNIST dataset. You have 60'000 28x28 pixel greyscale images of digits (0-9).\n",
        "\n",
        "**Content:**\n",
        "* load the original MNIST data\n",
        "* define a CNN in Keras\n",
        "* evaluation of the cross entropy loss function of the untrained CNN for all classes\n",
        "* implement the loss function yourself using the predicted probabilities and numpy\n",
        "\n",
        "\n",
        "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_04/nb_ch04_02.ipynb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEIS4WvpsT5t"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "First you load all the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y6S_hQX5oaBw",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# load required libraries:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Convolution2D, MaxPooling2D, Flatten , Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Likelihood if you have no clue\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "If you have no idea about the training dataset, your guess for every image would be 1/nr_of_classes. Calculate the NLL for that case."
      ],
      "metadata": {
        "id": "ePknaVwI0YBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "nr_of_classes=10\n",
        "-np.log(1/nr_of_classes) # aber das ist nicht der ganze NLL, oder? man müsste doch über die Klassen summieren, oder?"
      ],
      "metadata": {
        "id": "ZtsW-gxA0xx4",
        "outputId": "9c8c3da2-57f1-4963-8706-53737b5d4c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3025850929940455"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Likelihood of untrained CNN"
      ],
      "metadata": {
        "id": "mPHUVTZV1YF7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_3TS0CtJJb"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Loading and preparing the MNIST data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4sZ8lqFfoaB2",
        "outputId": "fb0e8d4b-4621-487f-c373-56789468570a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 28, 28, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train=x_train / 255 #divide by 255 so that they are in range 0 to 1\n",
        "X_train=np.reshape(X_train, (X_train.shape[0],28,28,1))\n",
        "Y_train=tensorflow.keras.utils.to_categorical(y_train,10) # one-hot encoding\n",
        "\n",
        "\n",
        "Y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaRFUEP8HJkq"
      },
      "source": [
        "## CNN model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JSfYQ4f1KYVp"
      },
      "outputs": [],
      "source": [
        "# here you define hyperparameter of the CNN\n",
        "batch_size = 128\n",
        "nb_classes = 10\n",
        "img_rows, img_cols = 28, 28\n",
        "kernel_size = (3, 3)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "pool_size = (2, 2) # pool size for max pooking 2D see below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "f0q16a2uIBkg"
      },
      "outputs": [],
      "source": [
        "# define CNN with 2 convolution blocks and 2 fully connected layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8,kernel_size,padding='same',input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(8, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Convolution2D(16, kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Convolution2D(16,kernel_size,padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(40))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(nb_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# compile model and intitialize weights\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dWx9gqJ6IUpZ",
        "outputId": "b35566da-f637-48bb-da68-2dcbe627921e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 8)         80        \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 28, 28, 8)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 8)         584       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 28, 28, 8)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 8)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 14, 14, 16)        1168      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 14, 14, 16)        2320      \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 7, 7, 16)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 40)                31400     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 40)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                410       \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 35962 (140.48 KB)\n",
            "Trainable params: 35962 (140.48 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# summarize model along with number of model weights\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-mY30BBI4LJ"
      },
      "source": [
        "Here you predict the probabilities for all images in the training data set. You did not train the network yet, therefore the probabilities will be around 10% for each class.\n",
        "\n",
        "meaning: the best you get is Zufall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "430DSTDIHJlP",
        "outputId": "813bd307-23b6-4cc8-a0c3-d503cae8ad1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Calculate the probailities for the training data\n",
        "Pred_prob = model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TTyxe7xMJUKC",
        "outputId": "db51f7a1-0907-4bef-9fa2-40bd9c19b2a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.10296943, 0.09263191, 0.10035602, 0.0961155 , 0.11109497,\n",
              "        0.0962236 , 0.09257398, 0.08844453, 0.10369592, 0.11589415],\n",
              "       [0.10268418, 0.09098673, 0.10038212, 0.10209884, 0.10864345,\n",
              "        0.09776118, 0.09255474, 0.08970154, 0.10368382, 0.1115034 ],\n",
              "       [0.10375823, 0.09088536, 0.0971105 , 0.09826044, 0.10639428,\n",
              "        0.10105354, 0.09742084, 0.09685304, 0.10034951, 0.10791427],\n",
              "       [0.10585216, 0.09597373, 0.10191199, 0.09743162, 0.10418429,\n",
              "        0.09507533, 0.09522017, 0.09180664, 0.1051529 , 0.10739113],\n",
              "       [0.09703027, 0.09535006, 0.0986185 , 0.0992071 , 0.10573916,\n",
              "        0.10074426, 0.09695639, 0.09739409, 0.1038496 , 0.10511065]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "Pred_prob[0:5] # Wkeit dass das Richtige vorausgesagt wird - wurde noch nicht optimiert, deshalb Zufallswerte!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XcqY_UbNYyP2",
        "outputId": "451b54ce-1711-412c-ed8d-11186775a2eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (60000, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "Pred_prob.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W46_Euob-ux"
      },
      "source": [
        "### Exercise : Calculate the loss function using numpy\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "*Exercise : Use numpy to calculate the value of the negative log-likelihood loss (=cross entropy) that you expect for the untrained CNN, which you have constructed above to discriminate between the 10 classes. Determine the cross entropy that results from the predicted probabilities (Pred_prob). To determine the cross entropy of the prediction, you can loop over each example and use its true label (Y_train) and the predicted probability for the true class. Do you get the cross entropy value that you have expected?*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "n-nSWXYadTft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4525602a-c994-4d2a-ec21-141786a25f9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1072.5604, 1100.5393, 1089.9813, 1088.987 , 1049.7903, 1092.1382,\n",
              "       1102.5281, 1106.1909, 1060.3293, 1038.0255], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Write your code here\n",
        "import numpy as np\n",
        "\n",
        "loss: float = 0\n",
        "for index in range(Pred_prob.shape[0])):\n",
        "    loss += (-1)/batch_size*np.log(Pred_prob[index][richtige Klasse!]) # ist batch_size wirklich das richtige 'n'?\n",
        "    #batch ist pro Durchgang, aber das Modell macht ja mehrere Durchgänge\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E06o34sUFDy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv1IEA74dPF6"
      },
      "source": [
        "Scroll down to see the solution.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM1EOk9WLkeh"
      },
      "source": [
        "In the next cell you calculate the cross entropy loss of each single image, then you sum up all individual losses and divide the sum with the nr of training examples. You take the negative of this result to get the NLL, also known as categorical cross entropy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "v9GkdLKcY5OU",
        "outputId": "cac7d6a4-322a-4971-c0da-2f54480902af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3025348495125773"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "loss=np.zeros(len(X_train))\n",
        "Y=np.argmax(Y_train,axis=1) # Y war hot encoded\n",
        "for i in range(0,len(X_train)):\n",
        "  loss[i]=np.log(Pred_prob[i][Y[i]])\n",
        "-np.sum(loss)/len(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJElA61ZMeZM"
      },
      "source": [
        "You get more a similar result as as you got with the model.evaluate function for the untrained CNN.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n60Ql16SLZac",
        "outputId": "85b9b00e-a60a-40fe-db7f-fca1e2b4d015",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 - 5s - loss: 2.3025 - accuracy: 0.0716 - 5s/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.302535057067871, 0.07164999842643738]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.evaluate(X_train, Y_train,verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# das ist gut für ein Modell, das noch nicht trainiert ist..."
      ],
      "metadata": {
        "id": "srrE01zdFYhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Change normalization\n",
        "<img src=\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/imgs/paper-pen.png\" width=\"60\" align=\"left\" />  \n",
        "\n",
        "Load the data again, but this time do not scale the data. Repeat the analysis. What is the result? Why is it a good idea to check the loss for untrained networks."
      ],
      "metadata": {
        "id": "kDiZFteB14_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scroll Down for solution\n",
        "# ganz oben nicht /255 teilen...\n",
        "# es kommen sehr schlechte Ergebnisse\n",
        "\n",
        "# inf sobald mal Null für die korrekte Klasse vorausgesagt wurde, dann wird der loss unendlich...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "FU565zi4yUS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the unscaled case, you do not necessarily get 1/10 for the probabilities, and thus the loss is usually larger than 2.3. So, the training starts worse than what you would get from pure guessing. This poor start not only leads to a higher initial loss but also increases training time. Effective initialization ensures a smoother and more efficient learning trajectory."
      ],
      "metadata": {
        "id": "E4sXJukR20iN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1S-pzgr3Lpe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}